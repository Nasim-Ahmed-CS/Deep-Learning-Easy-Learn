# Finally!!!
<p>One thing we need to clarify here. Though many frameworks are available for DL or ML-related tasks, we will consider only PyTorch for our space, as it is a simple and object-like paradigm.</p>

# What is PyTorch?
Willing to learn more about PyTorch? Visit <a href='https://pytorch.org/docs/stable/index.html'>PyTorch Documentation</a>

# Need the code
Watch the full code <a href='https://github.com/Nasim-Ahmed71/Deep-Learning-Easy-Learn/blob/main/Introduction%20to%20ML/1.Basic%20ML/Codes/Basic_ML_Linear_Regression.ipynb'>here</a>

# Python Code
```import torch
import torch.nn as nn
import numpy as np

########################################################################################
X = np.array([1.23, 2.43, 3.123, 4.345, 5.234, 6.24]).reshape(2,3) #this is to convert python list to a np array
X = torch.tensor(X , dtype = torch.float32) #this is for converting np array to a tensor
y = np.array([2.3, 3.32, 3.31, 2.22, 2.25, 4.54]).reshape(2,3)
y = torch.tensor(y, dtype = torch.float32)
########################################################################################

criterion = nn.MSELoss() #MSELoss is like the problem calculator diagnosis system of the docter
model = nn.Linear(3,3) #model, 1 output for 1 feature
optimizer = torch.optim.SGD(model.parameters(), lr=0.01) #to update the values of m and b

########################################################################################
epoch = 100
for i in range(epoch):
  optimizer.zero_grad() #this is for preparation of the doctor
  output = model(X)
  loss = criterion(y, output)
  print("Loss is : "+str(loss.item()))
  loss.backward()
  optimizer.step()


import matplotlib.pyplot as plt

epoch = 100
outputs = []

for i in range(epoch):
    optimizer.zero_grad()  # this is for preparation of the doctor
    output = model(X)
    loss = criterion(y, output)
    print("Loss is: " + str(loss.item()))
    loss.backward()
    optimizer.step()

    outputs.append(output.detach().numpy())  # Store the output predictions

# Plotting the output vs. input graph
plt.figure(figsize=(8, 6))
for output in outputs:
    plt.plot(X.numpy(), output, alpha=0.5)  # Plotting each output for each epoch

plt.scatter(X.numpy(), y.numpy(), color='red', label='Actual')  # Plotting the actual data points
plt.xlabel('Input (X)')
plt.ylabel('Output')
plt.title('Output vs. Input')
plt.legend()
plt.grid(True)
plt.show()
```

If you are afraid of the size, then follow the following-
```import torch
import torch.nn as nn
import torch.optim as optim

# Sample data
X = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)
y = torch.tensor([[2.0], [4.0], [6.0], [8.0]], dtype=torch.float32)

# Define a linear regression model
class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(1, 1)  # Input size: 1, Output size: 1

    def forward(self, x):
        return self.linear(x)

# Instantiate the model
model = LinearRegression()

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Training loop
num_epochs = 1000
for epoch in range(num_epochs):
    # Forward pass
    outputs = model(X)
    loss = criterion(outputs, y)
    
    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Testing
with torch.no_grad():
    predicted = model(torch.tensor([[5.0]], dtype=torch.float32))
    print(f'Prediction after training: {predicted.item():.4f}')
```


# Let's dig into the code
First of all, we have imported the necessary libraries using the import section. X and y are our given data by which we will find the best values of $m$ and $b$, and then we will calculate the value of y for a new instance of X with the help of these $m$ and $b$.
<b><p>1. Importing PyTorch Modules: </p></b>
```
import torch
import torch.nn as nn
import torch.optim as optim
```
<p>Here, we import the necessary modules from PyTorch. $torch$ is the main module, $nn$ contains neural network-related classes and functions, and $optim$ contains optimization algorithms.</p>

<b><p>2. Sample Data Definition:</p></b>
```
X = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)
y = torch.tensor([[2.0], [4.0], [6.0], [8.0]], dtype=torch.float32)
```
<p>Here, we define sample data $X$ and corresponding labels (aka targets) $y$. In this example, $X$ contains input features (a column vector with values $[1, 2, 3, 4]$) and $y$ contains corresponding target values (also a column vector with values $[2, 4, 6, 8]$).</p>

<b><p>3. Defining the Linear Regression Model:</p></b>
```
class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(1, 1)  # Input size: 1, Output size: 1

    def forward(self, x):
        return self.linear(x)
```
<p>This code defines a simple linear regression model using the $nn.Module$ class provided by PyTorch. The model consists of a single linear layer $(nn.Linear)$ which takes one input and produces one output.</p>

<b><p>4. Instantiating the Model:</p></b>
```
model = LinearRegression()
```
<p>Here, we create an instance of the $LinearRegression$ model.</p>
<p><b>5. Defining Loss Function and Optimizer:</b><br>We define Mean Squared Error (MSE) loss using $nn.MSELoss()$ and Stochastic Gradient Descent (SGD) optimizer using $optim.SGD()$. We pass $model.parameters()$ to the optimizer so that it knows which parameters to update during optimization.</p>
<p><b>6. Training Loop</b><br>In this loop, we iterate through a fixed number of epochs (num_epochs). For each epoch:
<ul><li>We perform a forward pass through the model to get predictions (outputs).</li>
<li>We calculate the loss between the predictions and the actual labels (y) using the defined loss function (criterion).</li>
<li>We set the gradients of all the model parameters to zero using optimizer.zero_grad().</li>
<li>We perform backpropagation to compute the gradients of the loss for the model parameters using loss.backward().</li>
<li>We update the model parameters using the optimizer (optimizer.step()).</li></ul>
Optionally, we print the loss every 100 epochs for monitoring training progress.</p>
<p><b>7. Testing</b><br>Here, we make a prediction using the trained model for a new input $5.0$. We use torch.no_grad() to deactivate gradient computation since we are only performing inference, not training. We print the predicted value.</p>

<p>Bored with code? Move to </p>
